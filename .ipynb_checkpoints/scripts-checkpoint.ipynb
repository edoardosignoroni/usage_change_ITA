{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0573e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edo/miniconda3/envs/align/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eea2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process():\n",
    "    def __init__(self, year):\n",
    "        self.year = year\n",
    "\n",
    "    def clean(self, user_stopwords):\n",
    "        \n",
    "\n",
    "        directory = f'./data/days_{self.year}/raw'\n",
    "\n",
    "        file_list =[]\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"_{}.txt\".format(self.year)):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        if os.path.isdir(f'./data/days_{self.year}/clean'):\n",
    "            shutil.rmtree(f'./data/days_{self.year}/clean')\n",
    "            os.mkdir(f'./data/days_{self.year}/clean')\n",
    "\n",
    "        init_time = time.time()\n",
    "        i=0\n",
    "        for file in file_list:\n",
    "            start_time = time.time()\n",
    "\n",
    "            file_name = str(os.path.basename(file)).strip('.txt')\n",
    "            print('Processing file: {}'.format(str(os.path.basename(file))))\n",
    "            with open(file,'r', encoding = 'utf-8') as in_file:\n",
    "                text = in_file.readlines()\n",
    "                \n",
    "                stopw = ' '+' |'.join(stopwords.words('italian'))\n",
    "                stopw_regex = re.compile\n",
    "                for line in text:\n",
    "                    \n",
    "                    #line = line.lower() #lowercasing\n",
    "                    line = re.sub(r'http/S+', ' URL', line) #Removes URLs\n",
    "                    #line = re.sub(r'[!\"‚Äù$%&()*+,-.//:;<=>?@/[/]^_`{|}~‚Ä¶¬ª‚Ä¢üòÄ‚ù§Ô∏èüòÄü§îü§£üò≠üòÖüôÑüòâ‚Äî]', '', line)   #Removes [] and other special chars\n",
    "                    #line = re.sub(r'#x200B', '', line)\n",
    "                    z=1\n",
    "\n",
    "                    line=line.split()\n",
    "                    for word in line:\n",
    "                        if word in stopwords.words('italian') or word in user_stopwords:\n",
    "                            line.remove(word)\n",
    "                        if len(word)>24:\n",
    "                            line.insert(line.index(word),'LONG')\n",
    "                            line.remove(word)\n",
    "                        if z%3000==0: #provides buffer for RAM usage\n",
    "                            text.insert(z, '/n/n')\n",
    "                        z+=1   \n",
    "\n",
    "                    line = ' '.join(line)\n",
    "                    with open('./data/days_{}/clean/{}.txt'.format(self.year, file_name), 'a+', encoding='utf-8') as out_file:\n",
    "                        print (line, file=out_file)\n",
    "                    \n",
    "            i+=1\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print ('Processed {} in {} - {}/{}'.format(file_name+'.txt', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), i, len(file_list)))\n",
    "            \n",
    "        total_time = time.time() - init_time\n",
    "        print ('Processed all {} of {} files in {}'.format(i, len(file_list), time.strftime(\"%H:%M:%S\", time.gmtime(total_time))))\n",
    "\n",
    "    def lemmatize(self):\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        directory = f'./data/days_{self.year}/clean'\n",
    "        processed_dir= f'./data/days_{self.year}/processed/processed_days'\n",
    "\n",
    "        file_list = []\n",
    "        done_list = []\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"_clean.txt\"):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        for entry in os.scandir(processed_dir):\n",
    "            if entry.path.endswith(\"sentencesL.txt\"):\n",
    "                entry_name = str(os.path.basename(entry).strip('sentencesL.txt')[:-1])\n",
    "                done_list.append(entry_name)\n",
    "\n",
    "        for done_entry in done_list:\n",
    "            done_path = done_entry+'.txt'\n",
    "            for entry in file_list:\n",
    "                if entry.path.endswith(done_path):\n",
    "                    file_list.remove(entry)\n",
    "\n",
    "        error = False\n",
    "        for f in done_list:\n",
    "            if f in file_list:\n",
    "                error = True\n",
    "                print(f, 'IN LIST! ERROR!')\n",
    "        if error:\n",
    "            quit()\n",
    "        else:\n",
    "            print(\"Lists OK!\")\n",
    "\n",
    "        print (len(file_list))\n",
    "\n",
    "        if len(file_list) == 0:\n",
    "            print('ALL DONE!')\n",
    "        else:\n",
    "            init_time = time.time()\n",
    "\n",
    "            nlp = stanza.Pipeline(lang='it', processors='tokenize, mwt, pos, lemma', tokenize_batch_size=25, mwt_batch_size=25, pos_batch_size=250, lemma_batch_size=25, lemma_max_dec_len=25, logging_level='WARN', use_gpu=True)\n",
    "            i=0\n",
    "            for file in file_list:\n",
    "                with open(file, 'r', encoding='utf-8') as in_file:\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    file_name = str(os.path.basename(file)[:-4])\n",
    "                    print('Processing file: {}'.format(file_name+'.txt'))\n",
    "                    text = in_file.read()\n",
    "                    doc = nlp(text)\n",
    "\n",
    "                    with open('./data/days_{}/processed/processed_days/{}_sentencesL.txt'.format(self.year, file_name), 'w+', encoding = 'utf-8') as out_file:\n",
    "\n",
    "                        for s in doc.sentences:\n",
    "                            s_list = []\n",
    "                            for w in s.words:\n",
    "                                s_list.append(w.text)\n",
    "\n",
    "                            print (\" \".join(s_list), file=out_file)\n",
    "                            print (\"\\n\", file=out_file)\n",
    "\n",
    "                i+=1\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                print ('Processed {} in {} - {}/{}'.format(file_name+'.txt', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), i, len(file_list)))\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "            total_time = time.time() - init_time\n",
    "            print ('Processed all {} of {} files in {}'.format(i, len(file_list), time.strftime(\"%H:%M:%S\", time.gmtime(total_time))))\n",
    "\n",
    "    def preprocess(self, user_stopwords):\n",
    "        self.clean(user_stopwords)\n",
    "        self.lemmatize()\n",
    "        self.join()\n",
    "        \n",
    "    def join(self):\n",
    "        directory = f'./data/days_{self.year}/processed'\n",
    "\n",
    "        file_list =[]\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"{}_clean_sentencesL.txt\".format(self.year)):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt'):\n",
    "            os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt')\n",
    "\n",
    "        i=0\n",
    "        for file in file_list:\n",
    "\n",
    "            with open(file, 'r', encoding='utf-8') as small_file:\n",
    "                text = small_file.read().lower()\n",
    "                with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'a+', encoding='utf-8') as big_file:\n",
    "                    print (text, file=big_file)\n",
    "                    print ('/n/n', file=big_file)\n",
    "            i+=1\n",
    "            print ('Processed {} of {} files'.format(i, len(file_list)))\n",
    "   \n",
    "    def freqs(self):\n",
    "\n",
    "        freqs = {}\n",
    "\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "\n",
    "            for line in text:\n",
    "                line = line.split()\n",
    "                for word in line:\n",
    "                    if word not in freqs.keys():\n",
    "                        freqs[word] = 1\n",
    "                    else:\n",
    "                        freqs[word] += 1\n",
    "\n",
    "        freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "        df = pd.DataFrame(freqs.items(), columns=['Lemma', 'Count'])\n",
    "\n",
    "        with open(f\"./data/days_{self.year}/freqs_{self.year}.tsv\", 'w+') as out_file:\n",
    "            df.to_csv(out_file, sep='\\t')\n",
    "\n",
    "        return freqs\n",
    "    \n",
    "    def chunks(self, lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "    \n",
    "    def prepostprocess(self):\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "    \n",
    "        if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt'):\n",
    "            os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt')\n",
    "            \n",
    "            with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt', 'a+') as out_file:\n",
    "                for line in text:\n",
    "                    for chunk in self.chunks(line.split(), 25):\n",
    "                        print(' '.join(chunk), file=out_file)\n",
    "\n",
    "    def filter_rare(self, threshold):\n",
    "        \n",
    "        freqs = self.freqs()\n",
    "        \n",
    "        #unique_file = open('./data/parole.txt', 'r')\n",
    "        #unique = unique_file.readlines()\n",
    "\n",
    "        rare = [key for key,value in freqs.items() if value <= threshold]\n",
    "                \n",
    "        print(f\"Filtering {len(rare)} words with freq lower than {threshold}\")\n",
    "        \n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "            \n",
    "            if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt'):\n",
    "                os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt')\n",
    "                \n",
    "            with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt', 'a+') as out_file:\n",
    "                i = 0\n",
    "                for line in text:\n",
    "                    start = time.time()\n",
    "                    print(f\"Filtering line {i} out of {len(text)}\")\n",
    "                    for word in line.split():\n",
    "                        if (word in rare): # and (word not in unique): # Too long\n",
    "                            line = re.sub(word, '', line)\n",
    "                    print(line, file=out_file)\n",
    "                    end = time.time()\n",
    "                    print(time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "                    if i%10 == 0:\n",
    "                        clear_output(wait=True)\n",
    "                        print(f\"Filtering {len(rare)} words with freq lower than {threshold}\")\n",
    "                    i+=1\n",
    "                    \n",
    "    def postprocess(self, rare_threshold):\n",
    "        self.filter_rare(rare_threshold)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96c675d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def split(in_file, out_path, split_len):\n",
    "    \n",
    "    file_name = os.path.basename(in_file)\n",
    "    save_dir = f'{out_path}/{file_name}_parts'\n",
    "    \n",
    "    if os.path.isdir(save_dir):\n",
    "        shutil.rmtree(save_dir)\n",
    "    \n",
    "    os.mkdir(save_dir)\n",
    "       \n",
    "    with open(in_file, 'r') as in_file:\n",
    "        \n",
    "        data = in_file.readlines()\n",
    "        num_lines = len(data)\n",
    "    \n",
    "        text_data = []\n",
    "        file_count = 0\n",
    "\n",
    "        for sample in data:\n",
    "            sample = sample.replace('\\n', '')\n",
    "            text_data.append(sample)\n",
    "            if len(text_data) == split_len:\n",
    "                save_path = f'{save_dir}/{file_name}_{file_count}.txt'\n",
    "                \n",
    "                with open(save_path, 'w+', encoding='utf-8') as fp:\n",
    "                    fp.write('\\n'.join(text_data))\n",
    "\n",
    "                text_data = []\n",
    "                file_count += 1\n",
    "    \n",
    "    paths = [str(x) for x in Path(f'{out_path}/{file_name}_parts').glob('**/*.txt')]\n",
    "    return paths\n",
    "\n",
    "def lemmatize(in_file, out_dir):\n",
    "    load_model = spacy.load('it_core_news_lg', disable=['parser', 'ner'])\n",
    "    \n",
    "    file_name = os.path.basename(in_file)\n",
    "    if not os.path.isdir(out_dir):\n",
    "        os.mkdir(out_dir)\n",
    "    \n",
    "    with open(in_file, 'r') as in_file, open(f'{out_dir}/{file_name}.lemmas', 'w+') as out_file:\n",
    "        text = in_file.read()\n",
    "        doc = load_model(text)\n",
    "        lemmas = ' '.join([token.lemma_ for token in doc])\n",
    "        print(lemmas, file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ad5b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m s \u001b[38;5;241m=\u001b[39m split(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test/days_2019_clean.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5000\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m s:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./test/2019\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(in_file, out_dir)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(in_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m in_file, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.lemmas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m out_file:\n\u001b[1;32m     46\u001b[0m     text \u001b[38;5;241m=\u001b[39m in_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 47\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([token\u001b[38;5;241m.\u001b[39mlemma_ \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc])\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(lemmas, file\u001b[38;5;241m=\u001b[39mout_file)\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/spacy/language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnO\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39malloc((\u001b[38;5;241m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m batch_id \u001b[38;5;241m=\u001b[39m Tok2VecListener\u001b[38;5;241m.\u001b[39mget_batch_id(docs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m listener \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlisteners:\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/with_array.py:38\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/with_array.py:73\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     71\u001b[0m lengths \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39masarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[1;32m     72\u001b[0m Xf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(Xs, pad\u001b[38;5;241m=\u001b[39mpad)\n\u001b[0;32m---> 73\u001b[0m Yf, get_dXf \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dYs: ListXd) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListXd:\n\u001b[1;32m     76\u001b[0m     dYf \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mflatten(dYs, pad\u001b[38;5;241m=\u001b[39mpad)\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output \u001b[38;5;241m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] \u001b[38;5;241m+\u001b[39m Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/align/lib/python3.8/site-packages/thinc/layers/maxout.py:52\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     50\u001b[0m Y \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape1f(b, nO \u001b[38;5;241m*\u001b[39m nP)\n\u001b[1;32m     51\u001b[0m Z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mreshape3f(Y, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], nO, nP)\n\u001b[0;32m---> 52\u001b[0m best, which \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(d_best: OutT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InT:\n\u001b[1;32m     55\u001b[0m     dZ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mbackprop_maxout(d_best, which, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s = split('./test/days_2019_clean.txt', './test', 5000)\n",
    "\n",
    "for path in s:\n",
    "    lemmatize(path, './test/2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8324d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f959b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTester:\n",
    "    def __init__(self, year, n_examples, n_iterations):\n",
    "        self.year = year\n",
    "        self.n_examples = n_examples\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "    def load_text(self):\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.read()\n",
    "            text = text.split()\n",
    "            \n",
    "            return text\n",
    "    def collect_examples(self, text):\n",
    "        indexes = random.sample(range(len(text)), self.n_examples)\n",
    "        examples = [text[index] for index in indexes]\n",
    "        \n",
    "        return examples\n",
    "        \n",
    "    def compare(self):\n",
    "        with open('./data/parole.txt', 'r') as dictionary:\n",
    "            dictionary = dictionary.readlines()\n",
    "            unique = [word.strip('\\n') for word in dictionary]\n",
    "            text = self.load_text()\n",
    "            examples = self.collect_examples(text)\n",
    "            results = []\n",
    "            errors = []\n",
    "            for example in examples:\n",
    "                if example in unique:\n",
    "                    results.append(1)\n",
    "                else:\n",
    "                    results.append(0)\n",
    "                    with open('./data/lemma_errors.txt', 'a+') as errors:\n",
    "                        print(example, file=errors)\n",
    "            perc = (sum(results)/self.n_examples)*100\n",
    "        \n",
    "        return perc\n",
    "  \n",
    "    def run(self):\n",
    "                \n",
    "        seeds = random.sample(range(9999), self.n_iterations)\n",
    "        iterations = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            random.seed(seed)\n",
    "            iterations.append(self.compare())\n",
    "        \n",
    "        average = (sum(iterations)/len(iterations))\n",
    "        \n",
    "        print (f\"Values for {self.n_iterations}:\")\n",
    "        print(iterations)\n",
    "        print (f\"Average value: {average}\")\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a073d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for 10:\n",
      "[93.0, 88.0, 90.0, 91.0, 86.0, 89.0, 84.0, 88.0, 95.0, 87.0]\n",
      "Average value: 89.1\n"
     ]
    }
   ],
   "source": [
    "LemmaTester(2019, 100, 10).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873e5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 880 words with freq lower than 5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/days_test/processed/days_test_sentencesL_prepost.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m user_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbla\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper√≥\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos√≠\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maaaah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maaah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahhhh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoooo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahahahaha\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhahahah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmhh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahahha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmmh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meheh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mohi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehe\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#Process(year).prepostprocess()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mProcess.postprocess\u001b[0;34m(self, rare_threshold)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, rare_threshold):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_rare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrare_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mProcess.filter_rare\u001b[0;34m(self, threshold)\u001b[0m\n\u001b[1;32m    208\u001b[0m         rare\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rare)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words with freq lower than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/days_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed/days_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_sentencesL_prepost.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m in_file:\n\u001b[1;32m    213\u001b[0m     text \u001b[38;5;241m=\u001b[39m in_file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/days_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/processed/days_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sentencesL_post.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/days_test/processed/days_test_sentencesL_prepost.txt'"
     ]
    }
   ],
   "source": [
    "years=['test']\n",
    "user_stopwords = ['bla', 'per√≥', 'cos√≠', 'bon', 'ehm', 'bhe', 'uh', 'aaaah', 'aaah', 'ahhhh', 'noooo', 'ahahahaha',\n",
    "                  'hahahah', 'mmh', 'mhh', 'ahahha', 'mmmh', 'ah', 'ehh', 'eheh', 'ohi', 'ehe']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    #Process(year).prepostprocess()\n",
    "    Process(year).postprocess(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3da44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   Lemma   Count\n",
      "0           0  essere  450312\n",
      "1           1      il  249948\n",
      "2           2    fare  233447\n",
      "3           3   avere  221198\n",
      "4           4  potere  130037\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./days_2019/freqs_2019.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m----> 4\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(data\u001b[38;5;241m=\u001b[39mdf, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.data/days_2019/freqs_2019.txt', sep='\\t')\n",
    "print(df.head())\n",
    "\n",
    "sns.lineplot(data=df, x='Unnamed: 0', y='Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742c5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
