{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0573e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edo/miniconda3/envs/align/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eea2d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Process():\n",
    "    def __init__(self, year):\n",
    "        self.year = year\n",
    "\n",
    "    def clean(self, user_stopwords):\n",
    "        \n",
    "\n",
    "        directory = f'./data/days_{self.year}/raw'\n",
    "\n",
    "        file_list =[]\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"_{}.txt\".format(self.year)):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        if os.path.isdir(f'./data/days_{self.year}/clean'):\n",
    "            shutil.rmtree(f'./data/days_{self.year}/clean')\n",
    "            os.mkdir(f'./data/days_{self.year}/clean')\n",
    "\n",
    "        init_time = time.time()\n",
    "        i=0\n",
    "        for file in file_list:\n",
    "            start_time = time.time()\n",
    "\n",
    "            file_name = str(os.path.basename(file)).strip('.txt')\n",
    "            print('Processing file: {}'.format(str(os.path.basename(file))))\n",
    "            with open(file,'r', encoding = 'utf-8') as in_file:\n",
    "                text = in_file.readlines()\n",
    "                \n",
    "                stopw = ' '+' |'.join(stopwords.words('italian'))\n",
    "                stopw_regex = re.compile\n",
    "                for line in text:\n",
    "                    \n",
    "                    #line = line.lower() #lowercasing\n",
    "                    line = re.sub(r'http/S+', ' URL', line) #Removes URLs\n",
    "                    #line = re.sub(r'[!\"‚Äù$%&()*+,-.//:;<=>?@/[/]^_`{|}~‚Ä¶¬ª‚Ä¢üòÄ‚ù§Ô∏èüòÄü§îü§£üò≠üòÖüôÑüòâ‚Äî]', '', line)   #Removes [] and other special chars\n",
    "                    #line = re.sub(r'#x200B', '', line)\n",
    "                    z=1\n",
    "\n",
    "                    line=line.split()\n",
    "                    for word in line:\n",
    "                        if word in stopwords.words('italian') or word in user_stopwords:\n",
    "                            line.remove(word)\n",
    "                        if len(word)>24:\n",
    "                            line.insert(line.index(word),'LONG')\n",
    "                            line.remove(word)\n",
    "                        if z%3000==0: #provides buffer for RAM usage\n",
    "                            text.insert(z, '/n/n')\n",
    "                        z+=1   \n",
    "\n",
    "                    line = ' '.join(line)\n",
    "                    with open('./data/days_{}/clean/{}.txt'.format(self.year, file_name), 'a+', encoding='utf-8') as out_file:\n",
    "                        print (line, file=out_file)\n",
    "                    \n",
    "            i+=1\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print ('Processed {} in {} - {}/{}'.format(file_name+'.txt', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), i, len(file_list)))\n",
    "            \n",
    "        total_time = time.time() - init_time\n",
    "        print ('Processed all {} of {} files in {}'.format(i, len(file_list), time.strftime(\"%H:%M:%S\", time.gmtime(total_time))))\n",
    "\n",
    "    def lemmatize(self):\n",
    "\n",
    "        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "        directory = f'./data/days_{self.year}/clean'\n",
    "        processed_dir= f'./data/days_{self.year}/processed/processed_days'\n",
    "\n",
    "        file_list = []\n",
    "        done_list = []\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"_clean.txt\"):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        for entry in os.scandir(processed_dir):\n",
    "            if entry.path.endswith(\"sentencesL.txt\"):\n",
    "                entry_name = str(os.path.basename(entry).strip('sentencesL.txt')[:-1])\n",
    "                done_list.append(entry_name)\n",
    "\n",
    "        for done_entry in done_list:\n",
    "            done_path = done_entry+'.txt'\n",
    "            for entry in file_list:\n",
    "                if entry.path.endswith(done_path):\n",
    "                    file_list.remove(entry)\n",
    "\n",
    "        error = False\n",
    "        for f in done_list:\n",
    "            if f in file_list:\n",
    "                error = True\n",
    "                print(f, 'IN LIST! ERROR!')\n",
    "        if error:\n",
    "            quit()\n",
    "        else:\n",
    "            print(\"Lists OK!\")\n",
    "\n",
    "        print (len(file_list))\n",
    "\n",
    "        if len(file_list) == 0:\n",
    "            print('ALL DONE!')\n",
    "        else:\n",
    "            init_time = time.time()\n",
    "\n",
    "            nlp = stanza.Pipeline(lang='it', processors='tokenize, mwt, pos, lemma', tokenize_batch_size=25, mwt_batch_size=25, pos_batch_size=250, lemma_batch_size=25, lemma_max_dec_len=25, logging_level='WARN', use_gpu=True)\n",
    "            i=0\n",
    "            for file in file_list:\n",
    "                with open(file, 'r', encoding='utf-8') as in_file:\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    file_name = str(os.path.basename(file)[:-4])\n",
    "                    print('Processing file: {}'.format(file_name+'.txt'))\n",
    "                    text = in_file.read()\n",
    "                    doc = nlp(text)\n",
    "\n",
    "                    with open('./data/days_{}/processed/processed_days/{}_sentencesL.txt'.format(self.year, file_name), 'w+', encoding = 'utf-8') as out_file:\n",
    "\n",
    "                        for s in doc.sentences:\n",
    "                            s_list = []\n",
    "                            for w in s.words:\n",
    "                                s_list.append(w.text)\n",
    "\n",
    "                            print (\" \".join(s_list), file=out_file)\n",
    "                            print (\"\\n\", file=out_file)\n",
    "\n",
    "                i+=1\n",
    "                elapsed_time = time.time() - start_time\n",
    "\n",
    "                print ('Processed {} in {} - {}/{}'.format(file_name+'.txt', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)), i, len(file_list)))\n",
    "                clear_output(wait=True)\n",
    "                \n",
    "            total_time = time.time() - init_time\n",
    "            print ('Processed all {} of {} files in {}'.format(i, len(file_list), time.strftime(\"%H:%M:%S\", time.gmtime(total_time))))\n",
    "\n",
    "    def preprocess(self, user_stopwords):\n",
    "        self.clean(user_stopwords)\n",
    "        self.lemmatize()\n",
    "        self.join()\n",
    "        \n",
    "    def join(self):\n",
    "        directory = f'./data/days_{self.year}/processed'\n",
    "\n",
    "        file_list =[]\n",
    "\n",
    "        for entry in os.scandir(directory):\n",
    "            if entry.path.endswith(\"{}_clean_sentencesL.txt\".format(self.year)):\n",
    "                file_list.append(entry)\n",
    "\n",
    "        if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt'):\n",
    "            os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt')\n",
    "\n",
    "        i=0\n",
    "        for file in file_list:\n",
    "\n",
    "            with open(file, 'r', encoding='utf-8') as small_file:\n",
    "                text = small_file.read().lower()\n",
    "                with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'a+', encoding='utf-8') as big_file:\n",
    "                    print (text, file=big_file)\n",
    "                    print ('/n/n', file=big_file)\n",
    "            i+=1\n",
    "            print ('Processed {} of {} files'.format(i, len(file_list)))\n",
    "   \n",
    "    def freqs(self):\n",
    "\n",
    "        freqs = {}\n",
    "\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "\n",
    "            for line in text:\n",
    "                line = line.split()\n",
    "                for word in line:\n",
    "                    if word not in freqs.keys():\n",
    "                        freqs[word] = 1\n",
    "                    else:\n",
    "                        freqs[word] += 1\n",
    "\n",
    "        freqs = dict(sorted(freqs.items(), key=lambda item: item[1], reverse=True))\n",
    "        df = pd.DataFrame(freqs.items(), columns=['Lemma', 'Count'])\n",
    "\n",
    "        with open(f\"./data/days_{self.year}/freqs_{self.year}.tsv\", 'w+') as out_file:\n",
    "            df.to_csv(out_file, sep='\\t')\n",
    "\n",
    "        return freqs\n",
    "    \n",
    "    def chunks(self, lst, n):\n",
    "        \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "        for i in range(0, len(lst), n):\n",
    "            yield lst[i:i + n]\n",
    "    \n",
    "    def prepostprocess(self):\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "    \n",
    "        if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt'):\n",
    "            os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt')\n",
    "            \n",
    "            with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt', 'a+') as out_file:\n",
    "                for line in text:\n",
    "                    for chunk in self.chunks(line.split(), 25):\n",
    "                        print(' '.join(chunk), file=out_file)\n",
    "\n",
    "    def filter_rare(self, threshold):\n",
    "        \n",
    "        freqs = self.freqs()\n",
    "        \n",
    "        #unique_file = open('./data/parole.txt', 'r')\n",
    "        #unique = unique_file.readlines()\n",
    "\n",
    "        rare = [key for key,value in freqs.items() if value <= threshold]\n",
    "                \n",
    "        print(f\"Filtering {len(rare)} words with freq lower than {threshold}\")\n",
    "        \n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_prepost.txt', 'r') as in_file:\n",
    "            text = in_file.readlines()\n",
    "            \n",
    "            if os.path.isfile(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt'):\n",
    "                os.remove(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt')\n",
    "                \n",
    "            with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL_post.txt', 'a+') as out_file:\n",
    "                i = 0\n",
    "                for line in text:\n",
    "                    start = time.time()\n",
    "                    print(f\"Filtering line {i} out of {len(text)}\")\n",
    "                    for word in line.split():\n",
    "                        if (word in rare): # and (word not in unique): # Too long\n",
    "                            line = re.sub(word, '', line)\n",
    "                    print(line, file=out_file)\n",
    "                    end = time.time()\n",
    "                    print(time.strftime(\"%H:%M:%S\", time.gmtime(end-start)))\n",
    "                    if i%10 == 0:\n",
    "                        clear_output(wait=True)\n",
    "                        print(f\"Filtering {len(rare)} words with freq lower than {threshold}\")\n",
    "                    i+=1\n",
    "                    \n",
    "    def postprocess(self, rare_threshold):\n",
    "        self.filter_rare(rare_threshold)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c675d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "def lemmatize(in_file):\n",
    "    load_model = spacy.load('it_core_news_lg', disable=['parser', 'ner'])\n",
    "    \n",
    "    file_name = os.path.basename(in_file)\n",
    "    \n",
    "    with open(in_file, 'r') as in_file, open(f'{file_name}.lemmas', 'w+') as out_file:\n",
    "        text = in_file.read()\n",
    "        doc = load_model(text)\n",
    "        lemmas = ' '.join([token.lemma_ for token in doc])\n",
    "        print(lemmas, file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ad5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatize('./test/raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f8324d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f959b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTester:\n",
    "    def __init__(self, year, n_examples, n_iterations):\n",
    "        self.year = year\n",
    "        self.n_examples = n_examples\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "    def load_text(self):\n",
    "        with open(f'./data/days_{self.year}/processed/days_{self.year}_sentencesL.txt', 'r') as in_file:\n",
    "            text = in_file.read()\n",
    "            text = text.split()\n",
    "            \n",
    "            return text\n",
    "    def collect_examples(self, text):\n",
    "        indexes = random.sample(range(len(text)), self.n_examples)\n",
    "        examples = [text[index] for index in indexes]\n",
    "        \n",
    "        return examples\n",
    "        \n",
    "    def compare(self):\n",
    "        with open('./data/parole.txt', 'r') as dictionary:\n",
    "            dictionary = dictionary.readlines()\n",
    "            unique = [word.strip('\\n') for word in dictionary]\n",
    "            text = self.load_text()\n",
    "            examples = self.collect_examples(text)\n",
    "            results = []\n",
    "            errors = []\n",
    "            for example in examples:\n",
    "                if example in unique:\n",
    "                    results.append(1)\n",
    "                else:\n",
    "                    results.append(0)\n",
    "                    with open('./data/lemma_errors.txt', 'a+') as errors:\n",
    "                        print(example, file=errors)\n",
    "            perc = (sum(results)/self.n_examples)*100\n",
    "        \n",
    "        return perc\n",
    "  \n",
    "    def run(self):\n",
    "                \n",
    "        seeds = random.sample(range(9999), self.n_iterations)\n",
    "        iterations = []\n",
    "        \n",
    "        for seed in seeds:\n",
    "            random.seed(seed)\n",
    "            iterations.append(self.compare())\n",
    "        \n",
    "        average = (sum(iterations)/len(iterations))\n",
    "        \n",
    "        print (f\"Values for {self.n_iterations}:\")\n",
    "        print(iterations)\n",
    "        print (f\"Average value: {average}\")\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a073d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for 10:\n",
      "[93.0, 88.0, 90.0, 91.0, 86.0, 89.0, 84.0, 88.0, 95.0, 87.0]\n",
      "Average value: 89.1\n"
     ]
    }
   ],
   "source": [
    "LemmaTester(2019, 100, 10).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873e5bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 880 words with freq lower than 5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/days_test/processed/days_test_sentencesL_prepost.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m user_stopwords \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbla\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mper√≥\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcos√≠\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbon\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbhe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maaaah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maaah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahhhh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoooo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahahahaha\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhahahah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmhh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mahahha\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmmmh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mah\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meheh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mohi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mehe\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m#Process(year).prepostprocess()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mProcess.postprocess\u001b[0;34m(self, rare_threshold)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpostprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, rare_threshold):\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_rare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrare_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mProcess.filter_rare\u001b[0;34m(self, threshold)\u001b[0m\n\u001b[1;32m    208\u001b[0m         rare\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiltering \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rare)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m words with freq lower than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/days_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/processed/days_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_sentencesL_prepost.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m in_file:\n\u001b[1;32m    213\u001b[0m     text \u001b[38;5;241m=\u001b[39m in_file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/days_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/processed/days_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sentencesL_post.txt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/days_test/processed/days_test_sentencesL_prepost.txt'"
     ]
    }
   ],
   "source": [
    "years=['test']\n",
    "user_stopwords = ['bla', 'per√≥', 'cos√≠', 'bon', 'ehm', 'bhe', 'uh', 'aaaah', 'aaah', 'ahhhh', 'noooo', 'ahahahaha',\n",
    "                  'hahahah', 'mmh', 'mhh', 'ahahha', 'mmmh', 'ah', 'ehh', 'eheh', 'ohi', 'ehe']\n",
    "\n",
    "\n",
    "for year in years:\n",
    "    #Process(year).prepostprocess()\n",
    "    Process(year).postprocess(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3da44df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   Lemma   Count\n",
      "0           0  essere  450312\n",
      "1           1      il  249948\n",
      "2           2    fare  233447\n",
      "3           3   avere  221198\n",
      "4           4  potere  130037\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./days_2019/freqs_2019.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m----> 4\u001b[0m sns\u001b[38;5;241m.\u001b[39mlineplot(data\u001b[38;5;241m=\u001b[39mdf, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mUnnamed: 0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('.data/days_2019/freqs_2019.txt', sep='\\t')\n",
    "print(df.head())\n",
    "\n",
    "sns.lineplot(data=df, x='Unnamed: 0', y='Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742c5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
